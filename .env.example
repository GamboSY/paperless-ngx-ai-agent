# Paperless-NGX Configuration
PAPERLESS_URL=http://localhost:8000
PAPERLESS_TOKEN=your_api_token_here

# Ollama Configuration
OLLAMA_URL=http://192.168.2.139:11434

# LLM für Klassifizierung und Q&A
# Empfohlene Modelle (nach Genauigkeit sortiert):
# OLLAMA_MODEL=qwen2.5:32b-instruct       # BESTE Genauigkeit für Chat/Q&A
# OLLAMA_MODEL=qwen2.5:14b-instruct       # Guter Kompromiss (empfohlen)
# OLLAMA_MODEL=qwen2.5:7b-instruct        # Schneller, weniger genau
# OLLAMA_MODEL=llama3.2:3b                # Sehr schnell, einfache Aufgaben
OLLAMA_MODEL=qwen2.5:14b-instruct

# Embedding-Modell für Semantische Suche / Q&A
# Empfohlene Modelle (nach Qualität sortiert):
# EMBEDDING_MODEL=mxbai-embed-large       # BESTE Qualität (334M params) - empfohlen!
# EMBEDDING_MODEL=nomic-embed-text        # Gut, kleiner (137M params)
# EMBEDDING_MODEL=all-minilm              # Sehr klein, basic (23M params)
EMBEDDING_MODEL=nomic-embed-text

# Query Expansion für bessere Synonym-Erkennung
# USE_LLM_EXPANSION=true   # LLM generiert Synonyme dynamisch (langsamer, intelligenter)
# USE_LLM_EXPANSION=false  # Nutzt hardcoded Synonym-Liste (schneller, begrenzt)
USE_LLM_EXPANSION=false

# Multi-Query Approach für bessere Dokumenten-Suche
# USE_MULTI_QUERY=true   # Generiert alternative Fragen für bessere Ergebnisse (empfohlen!)
# USE_MULTI_QUERY=false  # Nutzt nur die Original-Frage (schneller, weniger genau)
USE_MULTI_QUERY=true
